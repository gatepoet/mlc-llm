.. MLC-LLM documentation master file, created by
   sphinx-quickstart on Mon May 15 14:00:22 2023.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

Welcome to MLC-LLM's documentation!
===================================

MLC LLM is a **universal solution** that allows **any language models** to be **deployed natively** on a diverse set of hardware backends and native applications, plus a **productive framework** for everyone to further optimize model performance for their own use cases.

Our mission is to **enable everyone to develop, optimize and deploy AI models natively on everyone's devices**.

Everything runs locally  with no server support and accelerated with local GPUs on your phone and laptops.

Supported platforms
-------------------

- iPhone, iPad
- Android phones
- Metal GPUs and Intel/ARM MacBooks
- AMD, Intel and NVIDIA GPUs via Vulkan on Windows and Linux
- NVIDIA GPUs via CUDA on Windows and Linux
- WebGPU on browsers (through companion project `WebLLM <https://github.com/mlc-ai/web-llm/tree/main>`__)


.. toctree::
   :maxdepth: 1
   :caption: User Guides

.. toctree::
   :maxdepth: 1
   :caption: Contribute to MLC-LLM

   contribute/community.rst

   


